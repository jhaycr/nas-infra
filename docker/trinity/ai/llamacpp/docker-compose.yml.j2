services:
  llama:
    image: ghcr.io/ggml-org/llama.cpp:light
    container_name: llamacpp
    restart: unless-stopped
    volumes:
      - /opt/llamacpp/models:/models
    ports:
      - target: 8080
        published: 8080
        mode: host